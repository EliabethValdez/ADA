#Assignment 1: predictive modeling on tabular data

The goal is not to "win", but to help you reflect on your model's results, see how others are doing, etc.
The leaderboard is based on the number of true positives in the top-250 cases. The results of your latest submission are used to rank
This means it is your job to keep track of different model versions / approaches / outputs in case you'd like to go back to an earlier result
The "public" leaderboard calculates your score for a predetermined set 50% of the instances in the test set
Later on, the leaderboard will be frozen (you'll be warned in advance and will have enough time) and the other 50% results (hidden leaderboard) will be shown
You should then reflect on both results and explain accordingly in your report. E.g. if you did well on the public leaderboard but not on the hidden one, what might have caused this?
Also take some time the reflect on the score measure being used here. Is the the measure you'd have chosen in this setting? How does the score compare to the AUC of your model?
Reflect on the features and target definition. What would you change? How would your model be able to do better? Do you agree with the way the target was established or not?
Your model needs to be build using R, Python (or Julia, if you so prefer). As an environment, you can use e.g. Jupyter (Notebook or Lab), RStudio, Google Colaboratory, Microsoft Azure Machine Learning Studio... and any additional library or package you want
The first part of your lab report should contain a clear overview of your whole modeling pipeline, including approach, exploratory analysis (if any), preprocessing, construction of model, set-up of validation and results of the model:

Feel free to include code fragments, tables, visualisations, etc.
In terms of formatting, be concise but complete. Some people prefer to use e.g. Jupyter notebooks to mix their code fragments with explanatory text, but LaTeX or Word or all fine, too
You can use any predictive technique/approach you want. The focus is not on the absolute outcome, but rather on the whole process: general setup, critical thinking, and the ability to get and validate an outcome
You're free to use unsupervised techniques for your data exploration part, too. When you decide to build a black box model, including some interpretability techniques to explain it is a plus
Any other assumptions or insights are thoughts can be included as well: the idea is to take what we've seen in class, get your hands dirty and reflect on what we've seen
Important: All groups should submit the results of their predictive model at least once to the leaderboard
All group members will have received a password through e-mail which you'll need to make submissions
